---
title: "Biomarkers of ASD"
subtitle: "Evaluating the impact of preprocessing, outlier detection, and selection strategies on ASD biomarker accuracy"
author: "Jeff Loomis, Sanil Katula, Shahil Patel, Max Chang"
date: last-modified
published-title: "Updated"
editor: visual
format:
  html:
    code-copy: true
code-copy: true
execute:
  message: false
  warning: false
  echo: false
  cache: true
---

```{r}
# load any other packages and read data here
required_packages <- c("tidyverse", "knitr", "DiagrammeR", "glmnet")

# Check and install missing packages
missing_packages <- required_packages[!(required_packages %in% installed.packages()[, "Package"])]

if (length(missing_packages) > 0) {
  install.packages(missing_packages, repos = "https://cloud.r-project.org", type = "binary")
}

# Update htmltools if needed (DiagrammeR dependency)
if (requireNamespace("htmltools", quietly = TRUE)) {
  if (packageVersion("htmltools") < "0.5.7") {
    install.packages("htmltools", repos = "https://cloud.r-project.org", type = "binary")
  }
}

library(tidyverse)
library(knitr)
```

## Abstract

In this proteomics analysis of 154 boys and 1,317 serum proteins, we examined biomarkers for distinguishing autism spectrum disorder (ASD) from typically developing (TD) controls and tested how different modeling choices affected results. After log10 transformation, z-scoring, and ±3 SD trimming, the baseline model used t-tests and Random Forests to select the top 10 proteins from each method, intersecting on five (DERM, RELT, MRC2, IgD, Cadherin-5). Logistic regression on these achieved 77.4% accuracy and 88.3% AUC. Three variations showed that (1) doing feature selection only on training data reduced performance (68.8% accuracy, 71.5% AUC), (2) expanding to the top 25 proteins improved it (83.9% accuracy, 95.0% AUC), and (3) using a fuzzy intersection (union) lowered it (67.7% accuracy, 78.3% AUC). A simpler LASSO model with cross-validation selected three proteins (DERM, IgD, eIF-4H), giving 67.7% accuracy and 75.8% AUC with higher specificity (86.7%) but lower sensitivity (50.0%). Overall, the five-protein baseline best balances sensitivity and specificity, while the LASSO model shows that a smaller panel can still perform reasonably well, highlighting a trade-off between simplicity and performance.

## Dataset

A total of 154 pediatric subjects were enrolled in the study, consisting of 76 boys with autism spectrum disorder (ASD; mean age 5.6 years, SD 1.7) and 78 typically developing (TD) boys (mean age 5.7 years, SD 2.0). The ethnic distribution was 73 White/Caucasian, 32 Hispanic/Latino, 17 African American/Black, 5 Asian or Pacific Islander, 23 of multiple or other ethnicities, and 4 not reported. Autism Diagnostic Observation Schedule (ADOS) scores were also collected for all participants.

Serum protein expression was measured using the SOMAscan® assay, which quantified 1,317 proteins in 150 μl serum from 154 participants. An additional 14 blinded duplicate samples (7 ASD, 7 TD) were run for quality control but not included in analyses. In the published study, 192 proteins were removed during quality control, leaving 1,125 for analysis. The dataset used here retains all 1,317 raw protein measurements prior to filtering. These proteins span a broad range of functional classes, including cytokines, growth factors, receptors, and signaling molecules. Protein levels were reported as relative fluorescence units (RFUs), reflecting their relative abundance.

Preprocessing for the present analysis involved three main steps applied to all protein measures: a log10 transform to stabilize variance and reduce skew, z-score standardization (mean 0, SD 1) to place proteins on a comparable scale, and trimming at ±3 SD to cap extreme outliers. After these transformations, the dataset was organized with group and ados leading, followed by the processed protein features, and saved in R binary format (`biomarker_clean`) for downstream statistical analysis.

## Summary of published analysis

The study by Hewitson et al. (2021) aimed to identify serum protein biomarkers that could distinguish children with autism spectrum disorder (ASD) from typically developing (TD) controls. Blood samples from 154 male children (76 ASD and 78 TD) were analyzed using the SOMAscan® 1.3K proteomic platform, which quantified 1,317 serum proteins. After quality control procedures excluded 192 proteins, the remaining 1,125 were log transformed, standardized to z-scores, and values beyond ±3 standard deviations were trimmed to reduce the influence of extreme outliers.

Three feature selection approaches were applied: two-sample t-tests comparing ASD and TD groups, random forest importance rankings, and Spearman correlations between protein levels and ADOS scores. From each method, the ten most informative proteins were selected, and the overlapping set across all three approaches consisted of IgD, suPAR, MAPK14, EPHB2, and DERM, which defined the core panel.

Logistic regression models were then used to evaluate classification performance and to assess whether adding additional proteins could improve accuracy. Four additional proteins, ROR1, GI24, eIF-4H, and ARSB, each improved classification performance when added to the five-protein core set. The combined nine-protein optimal panel achieved an AUC of 86% (sensitivity 83%, specificity 84%).

#### Workflow of Hewitson et al. (2021) Biomarker Analysis

```{r}
#| echo: false
library(DiagrammeR)

grViz("
digraph G {
  rankdir=TB;
  graph [splines=true, nodesep=0.6, ranksep=0.5];
  node  [shape=box, style=\"rounded,filled\", fillcolor=\"#F5F7FA\", color=\"#94A3B8\", fontname=\"Helvetica\"];
  edge  [color=\"#94A3B8\"];

  # Top row (data & preprocessing)
  A [label=\"Serum samples\\n154 boys (76 ASD, 78 TD)\"];
  B [label=\"SOMAscan 1.3K\\n1,317 proteins\"];
  C [label=\"Protein QC\\nremove 192\"];
  D [label=\"Retained\\n1,125 proteins\"];
  E [label=\"Preprocessing\\nlog10 transform, z-score, trim ±3 SD\"];

  # Second row (feature selection & modeling)
  M1 [label=\"t-tests\\n(ASD vs TD)\"];
  M2 [label=\"Random forest\\nimportance\"];
  M3 [label=\"Spearman correlation\\nwith ADOS\"];
  CORE [label=\"Overlap of top 10\\nfrom each method\\nIgD, suPAR, MAPK14, EPHB2, DERM\"];
  ADD4 [label=\"Add 4 proteins\\nROR1, GI24, eIF-4H, ARSB\"];
  LR [label=\"Logistic regression\\nmodeling\"];
  FINAL [label=\"Final classifier\\n9-protein panel\\nAUC 86% (sens 83%, spec 84%)\"];

  {rank=same; M1; M2; M3}

  A -> B -> C -> D -> E;
  E -> M1;
  E -> M2;
  E -> M3;

  M1 -> CORE;
  M2 -> CORE;
  M3 -> CORE;

  CORE -> ADD4 -> LR -> FINAL;
}
")
```

## Findings

Summarize your findings here. I've included some subheaders in a way that seems natural to me; you can structure this section however you like.

### Impact of preprocessing

#### Log-transformation of protein intensities

To assess the effect of log-transformation, I compared raw versus log10-transformed protein intensities.\
On the raw scale, most proteins were strongly right-skewed (median skew ≈ 2.0).\
After applying the log10 transform (with a small positive shift for non-positive values), distributions became substantially more symmetric (median skew ≈ 0.2).\
About 70 % of proteins showed reduced absolute skewness, and the relationship between the mean and standard deviation was much weaker, indicating partial variance stabilization.

Overall, the log10 transformation reduces skewness, tames extreme high values, and stabilizes variance—important for statistical models that assume approximate normality and homoscedasticity.

```{r}
#| echo: false
#| fig-cap: "Distributions of raw (left) and log10-transformed (right) protein intensities for a random sample of nine proteins."
#| layout-ncol: 2
knitr::include_graphics(c(
  "q1_log/hist_raw_sample.png",
  "q1_log/hist_log_sample.png"
))


#| echo: false
#| fig-cap: "Q–Q plots for the same proteins before (left) and after (right) log10 transformation, showing improved approximate normality."
#| layout-ncol: 2

knitr::include_graphics(c(
"q1_log/qq_raw_sample.png",
"q1_log/qq_log_sample.png"
))

#| echo: false
#| fig-cap: "Standard deviation versus mean for each protein on the raw (left) and log10 (right) scales. The strong coupling of mean and SD on the raw scale is reduced after log-transformation."
#| layout-ncol: 2

knitr::include_graphics(c(
"q1_log/mean_sd_raw.png",
"q1_log/mean_sd_log.png"
))
```

### Outliers Analysis

Outlier analysis of per-subject counts (\|z\| \> 3) showed that ASD and TD groups had similar medians (both 8.5; Wilcoxon p = 0.82), but TD participants exhibited greater dispersion with higher means, larger maxima, and a heavier right tail. Using Tukey’s 1.5×IQR rule, 9 of 78 TD participants (11.5%) were flagged as outliers compared to 4 of 76 ASD participants (5.3%). Overall, while the central tendency of outlier counts was similar across groups, extreme outlying subjects were more common in the TD group.

```{r}
#| echo: false

# -------------------
# Read headers
# -------------------
var_names <- read_csv('../data/biomarker-raw.csv',
                      col_names = FALSE,
                      n_max = 2,
                      col_select = -(1:2)) %>%
  t() %>%
  as_tibble() %>%
  rename(name = V1, abbreviation = V2) %>%
  na.omit()

# -------------------
# Load data (no trimming); log10 + z-score
# -------------------
biomarker_no_trim <- read_csv('../data/biomarker-raw.csv',
                              skip = 2,
                              col_select = -2L,
                              col_names = c('group', 'empty', pull(var_names, abbreviation), 'ados'),
                              na = c('-', '')
) %>%
  filter(!is.na(group)) %>%
  mutate(across(-c(group, ados),
                ~ scale(log10(.x))[, 1])) %>%
  select(group, ados, everything())
```

**Group summaries**\

We computed a per-subject outlier count as the number of proteins with (\|z\|\>3) after log10 transformation and z-scoring. The first table summarizes these counts by group (ASD vs TD), reporting (N), mean, median, SD, and range. Both groups had similar medians (8.5), but TD subjects showed greater variability, with a higher mean (17.6 compared to 13.3) and a wider range of values (0–157 versus 0–126). The second table shows how often subjects exceeded selected thresholds (≥10, 25, 50, 100, and 200 outliers). Across all thresholds, a larger proportion of TD participants were above each cutoff, indicating more frequent extreme outlier counts. Together, these summaries show that while the typical number of outliers per subject was comparable between groups, the TD distribution extended further to the right, with a slightly larger number of subjects showing unusually high outlier counts.

```{r}
# ------------------
# Per-subject outlier count (|z| > 3)
# -------------------
outlier_counts <- biomarker_no_trim %>%
  mutate(outlier_n = rowSums(across(-c(group, ados), ~ abs(.x) > 3), na.rm = TRUE)) %>%
  select(group, ados, outlier_n)

# -------------------
# Group summary (ASD vs TD)
# -------------------
summary_tbl <- outlier_counts %>%
  group_by(group) %>%
  summarise(
    N      = n(),
    Mean   = mean(outlier_n),
    Median = median(outlier_n),
    SD     = sd(outlier_n),
    Min    = min(outlier_n),
    Max    = max(outlier_n),
    .groups = "drop"
  ) %>%
  arrange(group) %>%
  mutate(across(c(Mean, Median, SD), ~ round(.x, 2)))

# -------------------
# Proportions above thresholds
# -------------------
thresholds <- c(10, 25, 50, 100, 200)
props_tbl <- map_dfr(
  thresholds,
  ~ outlier_counts %>%
    group_by(group) %>%
    summarise(
      threshold       = .x,
      num_at_or_above = sum(outlier_n >= .x),
      N               = n(),
      proportion      = num_at_or_above / N,
      .groups = "drop"
    )
) %>%
  arrange(threshold, group) %>%
  mutate(proportion = round(proportion, 3))

# Show tables
kable(summary_tbl,
            caption = "Per-subject outlier counts (|z| > 3) after log10 transform and z-scoring (no trimming)")

kable(props_tbl,
            caption = "Proportion of subjects with outlier counts at or above selected thresholds (|z| > 3)")
```

 

**Descriptive distributions (histogram & boxplot)**\

The histogram of per-subject outlier counts shows that both ASD and TD groups cluster around similar values, but the TD group has a heavier right tail, with more subjects showing very high counts. The boxplot confirms this: medians are identical across groups, but TD participants show greater spread and more extreme high values. Tukey-flagged outlier participants are visible as red points in the boxplot.

```{r}
#| echo: false

# -------------------
# Flag subject-level outliers (Tukey rule per group)
# -------------------
fences <- outlier_counts %>%
  group_by(group) %>%
  summarise(
    Q1  = quantile(outlier_n, 0.25, na.rm = TRUE),
    Q3  = quantile(outlier_n, 0.75, na.rm = TRUE),
    IQR = Q3 - Q1,
    lower = Q1 - 1.5 * IQR,
    upper = Q3 + 1.5 * IQR,
    .groups = "drop"
  )

# -------------------
# Histogram by group (faceted)
# -------------------
p_hist <- ggplot(outlier_counts, aes(x = outlier_n, fill = group)) +
  geom_histogram(bins = 30, alpha = 0.6) +
  facet_wrap(~ group, ncol = 1, scales = "free_y") +
  labs(
    title = "Histogram of Per-Subject Outlier Counts",
    x = "Number of Outliers (|z| > 3)",
    y = "Frequency"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

p_hist

outlier_flagged <- outlier_counts %>%
  left_join(fences, by = "group") %>%
  mutate(is_outlier = outlier_n < lower | outlier_n > upper)

# -------------------
# Boxplot with outliers highlighted
# -------------------
p_box <- ggplot(outlier_flagged, aes(x = group, y = outlier_n, color = is_outlier)) +
  geom_boxplot(outlier.shape = NA) +
  geom_jitter(width = 0.15, alpha = 0.7, size = 2) +
  scale_color_manual(values = c("FALSE" = "gray40", "TRUE" = "red")) +
  labs(
    title = "Per-Subject Outlier Counts by Group",
    x = "Group",
    y = "Number of Outliers (|z| > 3)",
    color = "Flagged as outlier participants"
  ) +
  theme_minimal()

p_box
```

 

**Group Comparisons of Outlier Counts**\

The Wilcoxon rank-sum test showed no significant difference in median outlier counts between ASD and TD participants ((W = 3028.5), (p = 0.82)). Using Tukey’s 1.5×IQR rule, 9 of 78 TD participants (11.5%) and 4 of 76 ASD participants (5.3%) were flagged as outliers. The chi-square test comparing these proportions was not significant, indicating that while extreme subjects occurred in both groups, they were slightly more common among TD participants without reaching statistical significance.

```{r}
#| echo: false

# --- Wilcoxon rank-sum (location) ---
wilcox_res <- wilcox.test(outlier_n ~ group, data = outlier_counts)
wilcox_tbl <- tibble::tibble(
  test    = "Wilcoxon rank-sum",
  W       = unname(wilcox_res$statistic),
  p_value = wilcox_res$p.value
)
knitr::kable(wilcox_tbl, caption = "Wilcoxon rank-sum test comparing per-subject outlier counts (ASD vs TD)")

# --- Flagged subjects per group (Tukey rule) ---
outlier_totals <- outlier_flagged %>%
  dplyr::count(group, is_outlier) %>%
  tidyr::complete(group, is_outlier, fill = list(n = 0)) %>%
  dplyr::group_by(group) %>%
  dplyr::mutate(prop = n / sum(n)) %>%
  dplyr::ungroup()

knitr::kable(outlier_totals,
             caption = "Flagged outlier participants by group (Tukey 1.5×IQR rule) with proportions")

# --- 2×2 table + chi-square test for flagged proportions ---
tab <- table(outlier_flagged$group, outlier_flagged$is_outlier)
chisq_res <- chisq.test(tab)

# format the contingency table for display
tab_df <- as.data.frame.matrix(tab) %>%
  tibble::rownames_to_column("Group")
knitr::kable(tab_df, caption = "2×2 contingency table: flagged (TRUE/FALSE) by group")

chisq_tbl <- tibble::tibble(
  test    = "Chi-square test of independence",
  statistic = unname(chisq_res$statistic),
  df        = unname(chisq_res$parameter),
  p_value   = chisq_res$p.value
)
knitr::kable(chisq_tbl, caption = "Chi-square test comparing flagged proportions across groups")
```

 

**Overall interpretation**\

-   Both groups share a similar central tendency (same medians, Wilcoxon not significant).\
-   The TD group shows greater dispersion and heavier tails, including more extreme subjects.\
-   Using Tukey’s rule, a larger proportion of TD participants were classified as outliers compared with ASD.

**Outlier Analysis Conclusion:** While typical participants look similar in ASD and TD, extremely outlying individuals are more common in the TD group.

## Methodological variations

We were then tasked to find out what happens to the model under these 3 modifications:

1\) Repeat the analysis but carry out the entire selection procedure on a training partition -- in other words, set aside some testing data at the very beginning and don't use it until you are evaluating accuracy at the very end.

2\) Choose a larger number (more than ten) of top predictive proteins using each selection method.

3\) Use a fuzzy intersection instead of a hard intersection to combine the sets of top predictive proteins across selection methods.

### Original Model:

The original model produces the following result:

```{r}
library(tidyverse)
library(infer)
library(randomForest)
library(tidymodels)
library(modelr)
library(yardstick)
load('../data/biomarker-clean.RData')

## MULTIPLE TESTING
####################

# function to compute tests
test_fn <- function(.df){
  t_test(.df, 
         formula = level ~ group,
         order = c('ASD', 'TD'),
         alternative = 'two-sided',
         var.equal = F)
}

ttests_out <- biomarker_clean %>%
  # drop ADOS score
  select(-ados) %>%
  # arrange in long format
  pivot_longer(-group, 
               names_to = 'protein', 
               values_to = 'level') %>%
  # nest by protein
  nest(data = c(level, group)) %>% 
  # compute t tests
  mutate(ttest = map(data, test_fn)) %>%
  unnest(ttest) %>%
  # sort by p-value
  arrange(p_value) %>%
  # multiple testing correction
  mutate(m = n(),
         hm = log(m) + 1/(2*m) - digamma(1),
         rank = row_number(),
         p.adj = m*hm*p_value/rank)

# select significant proteins
proteins_s1 <- ttests_out %>%
  slice_min(p.adj, n = 10) %>%
  pull(protein)

## RANDOM FOREST
##################

# store predictors and response separately
predictors <- biomarker_clean %>%
  select(-c(group, ados))

response <- biomarker_clean %>% pull(group) %>% factor()

# fit RF
set.seed(101422)
rf_out <- randomForest(x = predictors, 
                       y = response, 
                       ntree = 1000, 
                       importance = T)

# check errors
rf_out$confusion

# compute importance scores
proteins_s2 <- rf_out$importance %>% 
  as_tibble() %>%
  mutate(protein = rownames(rf_out$importance)) %>%
  slice_max(MeanDecreaseGini, n = 10) %>%
  pull(protein)

## LOGISTIC REGRESSION
#######################

# select subset of interest
# select subset of interest
proteins_sstar <- intersect(proteins_s1, proteins_s2)

biomarker_sstar <- biomarker_clean %>%
  select(group, any_of(proteins_sstar)) %>%
  # *** FIX: Create 'class' as a factor with explicit levels
  mutate(class = factor(ifelse(group == 'ASD', "ASD", "TD"), levels = c("TD", "ASD"))) %>%
  select(-group)

# partition into training and test set
set.seed(101422)
biomarker_split <- biomarker_sstar %>%
  initial_split(prop = 0.8)

# fit logistic regression model to training set
fit <- glm(class ~ ., 
           data = training(biomarker_split), 
           family = 'binomial')

# evaluate errors on test set
class_metrics <- metric_set(sensitivity, 
                            specificity, 
                            accuracy,
                            roc_auc)

testing(biomarker_split) %>%
  add_predictions(fit, type = 'response') %>%
  
  # Add a new column for the *predicted class* as a factor
  mutate(
    .pred_class = factor(ifelse(pred > 0.5, "ASD", "TD"), levels = c("TD", "ASD"))
  ) %>%
  
  # Call class_metrics using the column names
  class_metrics(truth = class,            # The true factor column
                estimate = .pred_class,   # The predicted factor column
                pred,                     # The raw probability (for roc_auc)
                event_level = 'second')   # 'second' level is "ASD"


```

-   RF OOB Error: Based on all 154 subjects.

-   Final Accuracy: 77.4%

-   Final AUC: 88.3%

    One problem from this method is that the feature selection was performed on the entire data set before splitting. This means the selection process "peeked" at the test data, choosing features that were already known to work well on it. This inflates the final score. The other methods would have a lower score than this.

### Variation 1:

Repeat the analysis but carry out the entire selection procedure on a training partition -- in other words, set aside some testing data at the very beginning and don't use it until you are evaluating accuracy at the very end.

```{r}
library(tidyverse)
library(infer)
library(randomForest)
library(tidymodels)
library(modelr)
library(yardstick)

load('../data/biomarker-clean.RData')

## 1. GLOBAL DATA PARTITION
###########################
# KEY CHANGE: Set aside test data at the very beginning.
# We use strata = group to ensure both classes are represented
# proportionally in the train and test sets.
set.seed(101422)
biomarker_split_global <- biomarker_clean %>%
  initial_split(prop = 0.8, strata = group)

# Create the two data frames we will use
train_data <- training(biomarker_split_global)
test_data  <- testing(biomarker_split_global)


## 2. MULTIPLE TESTING (on Training Data)
#########################################

# function to compute tests (no change)
test_fn <- function(.df){
  t_test(.df, 
         formula = level ~ group,
         order = c('ASD', 'TD'),
         alternative = 'two-sided',
         var.equal = F)
}

# KEY CHANGE: Perform t-tests *only* on the training data
ttests_out_train <- train_data %>% # <-- Use train_data
  # drop ADOS score
  select(-ados) %>%
  # arrange in long format
  pivot_longer(-group, 
               names_to = 'protein', 
               values_to = 'level') %>%
  # nest by protein
  nest(data = c(level, group)) %>% 
  # compute t tests
  mutate(ttest = map(data, test_fn)) %>%
  unnest(ttest) %>%
  # sort by p-value
  arrange(p_value) %>%
  # multiple testing correction
  mutate(m = n(),
         hm = log(m) + 1/(2*m) - digamma(1),
         rank = row_number(),
         p.adj = m*hm*p_value/rank)

# Select significant proteins *based only on training data*
proteins_s1_train <- ttests_out_train %>%
  slice_min(p.adj, n = 10) %>%
  pull(protein)


## 3. RANDOM FOREST (on Training Data)
#######################################

# KEY CHANGE: Store predictors and response *from the training data*
predictors_train <- train_data %>% # <-- Use train_data
  select(-c(group, ados))

response_train <- train_data %>% pull(group) %>% factor() # <-- Use train_data

# KEY CHANGE: Fit RF *only* on the training data
set.seed(101422)
rf_out_train <- randomForest(x = predictors_train, 
                             y = response_train, 
                             ntree = 1000, 
                             importance = T)

# check errors (this is the OOB error on the training set)
rf_out_train$confusion

# Compute importance scores *from the training RF*
proteins_s2_train <- rf_out_train$importance %>% 
  as_tibble() %>%
  mutate(protein = rownames(rf_out_train$importance)) %>%
  slice_max(MeanDecreaseGini, n = 10) %>%
  pull(protein)


## 4. LOGISTIC REGRESSION (Fit on Train, Evaluate on Test)
###########################################################

# KEY CHANGE: Select subset of interest *based on training set feature selection*
proteins_sstar_train <- intersect(proteins_s1_train, proteins_s2_train)

# KEY CHANGE: Create the final *training* set with *only* the selected features
glm_train_data <- train_data %>%
  select(group, any_of(proteins_sstar_train)) %>%
  # Use the factor logic you had
  mutate(class = factor(ifelse(group == 'ASD', "ASD", "TD"), levels = c("TD", "ASD"))) %>%
  select(-group)

# KEY CHANGE: Create the final *test* set with *only* the features selected during training
glm_test_data <- test_data %>%
  select(group, any_of(proteins_sstar_train)) %>%
  mutate(class = factor(ifelse(group == 'ASD', "ASD", "TD"), levels = c("TD", "ASD"))) %>%
  select(-group)

# Fit logistic regression model *only* to the new training set
fit_final <- glm(class ~ ., 
                 data = glm_train_data, # <-- Use glm_train_data
                 family = 'binomial')

# Define metrics
class_metrics <- metric_set(sensitivity, 
                            specificity, 
                            accuracy,
                            roc_auc)

# KEY CHANGE: Evaluate errors *only* on the new, held-out test set
glm_test_data %>% # <-- Use glm_test_data
  add_predictions(fit_final, type = 'response') %>%
  
  mutate(
    .pred_class = factor(ifelse(pred > 0.5, "ASD", "TD"), levels = c("TD", "ASD"))
  ) %>%
  
  # Call class_metrics using the column names
  class_metrics(truth = class,            # The true factor column
                estimate = .pred_class,   # The predicted factor column
                pred,                     # The raw probability (for roc_auc)
                event_level = 'second')   # 'second' level is "ASD"

```

-   RF OOB Error: Based on the smaller training set (122 subjects)

-   Final Accuracy: 68.8%

-   Final AUC: 71.5%

    The accuracy and AUC dropped a lot. This 68.8% accuracy is the honest and realistic score. The \~9-point drop in accuracy reveals the cost of data leakage. The original model was much more optimistic, and this model leads to more truthful results.

### Variation 2:

Choose a larger number (more than ten) of top predictive proteins using each selection method. Decided to use n=25 instead of n=10.

```{r}
library(tidyverse)
library(infer)
library(randomForest)
library(tidymodels)
library(modelr)
library(yardstick)
load('../data/biomarker-clean.RData')

## MULTIPLE TESTING
####################

# function to compute tests
test_fn <- function(.df){
  t_test(.df, 
         formula = level ~ group,
         order = c('ASD', 'TD'),
         alternative = 'two-sided',
         var.equal = F)
}

ttests_out <- biomarker_clean %>%
  # drop ADOS score
  select(-ados) %>%
  # arrange in long format
  pivot_longer(-group, 
               names_to = 'protein', 
               values_to = 'level') %>%
  # nest by protein
  nest(data = c(level, group)) %>% 
  # compute t tests
  mutate(ttest = map(data, test_fn)) %>%
  unnest(ttest) %>%
  # sort by p-value
  arrange(p_value) %>%
  # multiple testing correction
  mutate(m = n(),
         hm = log(m) + 1/(2*m) - digamma(1),
         rank = row_number(),
         p.adj = m*hm*p_value/rank)

# select significant proteins
proteins_s1 <- ttests_out %>%
  slice_min(p.adj, n = 25) %>%
  pull(protein)

## RANDOM FOREST
##################

# store predictors and response separately
predictors <- biomarker_clean %>%
  select(-c(group, ados))

response <- biomarker_clean %>% pull(group) %>% factor()

# fit RF
set.seed(101422)
rf_out <- randomForest(x = predictors, 
                       y = response, 
                       ntree = 1000, 
                       importance = T)

# check errors
rf_out$confusion

# compute importance scores
proteins_s2 <- rf_out$importance %>% 
  as_tibble() %>%
  mutate(protein = rownames(rf_out$importance)) %>%
  slice_max(MeanDecreaseGini, n = 25) %>%
  pull(protein)

## LOGISTIC REGRESSION
#######################

# select subset of interest
# select subset of interest
proteins_sstar <- intersect(proteins_s1, proteins_s2)

biomarker_sstar <- biomarker_clean %>%
  select(group, any_of(proteins_sstar)) %>%
  # *** FIX: Create 'class' as a factor with explicit levels
  mutate(class = factor(ifelse(group == 'ASD', "ASD", "TD"), levels = c("TD", "ASD"))) %>%
  select(-group)

# partition into training and test set
set.seed(101422)
biomarker_split <- biomarker_sstar %>%
  initial_split(prop = 0.8)

# fit logistic regression model to training set
fit <- glm(class ~ ., 
           data = training(biomarker_split), 
           family = 'binomial')

# evaluate errors on test set
class_metrics <- metric_set(sensitivity, 
                            specificity, 
                            accuracy,
                            roc_auc)

testing(biomarker_split) %>%
  add_predictions(fit, type = 'response') %>%
  
  # Add a new column for the *predicted class* as a factor
  mutate(
    .pred_class = factor(ifelse(pred > 0.5, "ASD", "TD"), levels = c("TD", "ASD"))
  ) %>%
  
  # Call class_metrics using the column names
  class_metrics(truth = class,            # The true factor column
                estimate = .pred_class,   # The predicted factor column
                pred,                     # The raw probability (for roc_auc)
                event_level = 'second')   # 'second' level is "ASD"

```

-   RF OOB Error: Based on all 154 subjects

-   Final Accuracy: 83.9%

-   Final AUC: 95.0%

-   How it was affected: The accuracy and AUC increased dramatically

    This suggests that the original model (n=10) was underfit. The features ranked 11-25 still contained valuable information. The model became more complex and could capture more of the underlying signal by including them which leads to better performance.

### Variation 3:

Use a fuzzy intersection instead of a hard intersection to combine the sets of top predictive proteins across selection methods.

The chnage was using union() for a "fuzzy intersection" instead of intersection.

```{r}
library(tidyverse)
library(infer)
library(randomForest)
library(tidymodels)
library(modelr)
library(yardstick)
load('../data/biomarker-clean.RData')

## MULTIPLE TESTING
####################

# function to compute tests
test_fn <- function(.df){
  t_test(.df, 
         formula = level ~ group,
         order = c('ASD', 'TD'),
         alternative = 'two-sided',
         var.equal = F)
}

ttests_out <- biomarker_clean %>%
  # drop ADOS score
  select(-ados) %>%
  # arrange in long format
  pivot_longer(-group, 
               names_to = 'protein', 
               values_to = 'level') %>%
  # nest by protein
  nest(data = c(level, group)) %>% 
  # compute t tests
  mutate(ttest = map(data, test_fn)) %>%
  unnest(ttest) %>%
  # sort by p-value
  arrange(p_value) %>%
  # multiple testing correction
  mutate(m = n(),
         hm = log(m) + 1/(2*m) - digamma(1),
         rank = row_number(),
         p.adj = m*hm*p_value/rank)

# select significant proteins
proteins_s1 <- ttests_out %>%
  slice_min(p.adj, n = 10) %>%
  pull(protein)

## RANDOM FOREST
##################

# store predictors and response separately
predictors <- biomarker_clean %>%
  select(-c(group, ados))

response <- biomarker_clean %>% pull(group) %>% factor()

# fit RF
set.seed(101422)
rf_out <- randomForest(x = predictors, 
                       y = response, 
                       ntree = 1000, 
                       importance = T)

# check errors
rf_out$confusion

# compute importance scores
proteins_s2 <- rf_out$importance %>% 
  as_tibble() %>%
  mutate(protein = rownames(rf_out$importance)) %>%
  slice_max(MeanDecreaseGini, n = 10) %>%
  pull(protein)

## LOGISTIC REGRESSION
#######################

# select subset of interest
# select subset of interest
proteins_sstar <- union(proteins_s1, proteins_s2)

biomarker_sstar <- biomarker_clean %>%
  select(group, any_of(proteins_sstar)) %>%
  # *** FIX: Create 'class' as a factor with explicit levels
  mutate(class = factor(ifelse(group == 'ASD', "ASD", "TD"), levels = c("TD", "ASD"))) %>%
  select(-group)

# partition into training and test set
set.seed(101422)
biomarker_split <- biomarker_sstar %>%
  initial_split(prop = 0.8)

# fit logistic regression model to training set
fit <- glm(class ~ ., 
           data = training(biomarker_split), 
           family = 'binomial')

# evaluate errors on test set
class_metrics <- metric_set(sensitivity, 
                            specificity, 
                            accuracy,
                            roc_auc)

testing(biomarker_split) %>%
  add_predictions(fit, type = 'response') %>%
  
  # Add a new column for the *predicted class* as a factor
  mutate(
    .pred_class = factor(ifelse(pred > 0.5, "ASD", "TD"), levels = c("TD", "ASD"))
  ) %>%
  
  # Call class_metrics using the column names
  class_metrics(truth = class,            # The true factor column
                estimate = .pred_class,   # The predicted factor column
                pred,                     # The raw probability (for roc_auc)
                event_level = 'second')   # 'second' level is "ASD"

```

-   RF OOB Error: Based on all 154 subjects

-   Final Accuracy: 67.7%

-   Final AUC: 78.3%

-   How it was affected: The accuracy and AUC dropped significantly

    This shows the danger of overfitting. The original intersection is a strict filter and keeps only high confidence features that both methods agreed on, while the union is a permissive filter and keeps features that either method liked. The union added more noise than signal and caused the model to drop in performance.

### Improved classifier

#### Motivation: Seeking a simpler biomarker panel

While the in-class analysis identified a 5-protein panel with strong classification accuracy (77.4% accuracy, 88.3% AUC), clinical translation of biomarker panels benefits from simplicity. Fewer proteins mean lower assay costs, faster turnaround times, and reduced technical variability. We therefore sought to identify a minimal protein panel that maintains comparable diagnostic performance.

#### Approach: LASSO regularization for automatic feature selection

We employed LASSO (Least Absolute Shrinkage and Selection Operator) regression, a penalized logistic regression method that performs automatic feature selection via L1 regularization. LASSO works by adding a penalty term proportional to the absolute value of coefficients, which shrinks less important features exactly to zero, effectively removing them from the model. This makes LASSO particularly well-suited for high-dimensional biomarker data where interpretability and parsimony are valued.

Our methodological workflow consisted of five steps:

1.  **Baseline replication**: We first replicated the in-class analysis to establish benchmark performance metrics using the same random seed and procedures.

2.  **Candidate expansion**: Rather than using only the strict intersection of t-test and Random Forest top-10 lists (which yielded 5 proteins), we used the *union* of both lists to create a larger candidate pool (15 proteins total). This gave LASSO more features to consider while still focusing on biologically relevant candidates.

3.  **LASSO feature selection**: We applied 10-fold cross-validation on the training set to select the optimal regularization strength (λ). Following best practices, we used the "1 standard error rule" (λ~1SE~) to favor a simpler model with slightly higher regularization, as it tends to generalize better and reduces overfitting.

4.  **Model evaluation**: We evaluated the LASSO-regularized logistic regression on the held-out test set and computed sensitivity, specificity, accuracy, and ROC-AUC.

5.  **Comparison**: We also fit a standard (unpenalized) logistic regression using only the LASSO-selected features to assess whether the regularization itself improved performance beyond simple feature selection.

#### Workflow diagram

```{r}
#| echo: false
library(DiagrammeR)

grViz("
digraph G {
  rankdir=TB;
  graph [splines=true, nodesep=0.6, ranksep=0.5];
  node  [shape=box, style='rounded,filled', fillcolor='#F5F7FA', color='#94A3B8', fontname='Helvetica'];
  edge  [color='#94A3B8'];

  # Top row
  A [label='Preprocessed data\\n(biomarker_clean)\\n154 subjects, 1,317 proteins'];
  
  # Feature selection
  M1 [label='t-tests\\nTop 10 proteins'];
  M2 [label='Random Forest\\nTop 10 proteins'];
  
  # Baseline vs New
  BASELINE [label='Baseline approach\\nIntersection\\n5 proteins', fillcolor='#FEF3C7'];
  UNION [label='LASSO approach\\nUnion (candidate pool)\\n15 proteins', fillcolor='#DBEAFE'];
  
  # LASSO
  LASSO [label='LASSO regression\\n10-fold CV\\nλ selection (1SE rule)', fillcolor='#DBEAFE'];
  SELECTED [label='LASSO-selected features\\n3 proteins\\nDERM, IgD, eIF-4H', fillcolor='#DBEAFE'];
  
  # Final models
  BASELINE_GLM [label='Baseline GLM\\n5 proteins\\nAcc: 77.4%, AUC: 88.3%', fillcolor='#FEF3C7'];
  LASSO_GLM [label='LASSO-regularized GLM\\n3 proteins\\nAcc: 67.7%, AUC: 75.8%', fillcolor='#DBEAFE'];
  
  {rank=same; M1; M2}
  {rank=same; BASELINE; UNION}

  A -> M1;
  A -> M2;
  
  M1 -> BASELINE;
  M2 -> BASELINE;
  
  M1 -> UNION;
  M2 -> UNION;
  
  BASELINE -> BASELINE_GLM;
  
  UNION -> LASSO;
  LASSO -> SELECTED;
  SELECTED -> LASSO_GLM;
}
")
```

#### Results

```{r}
#| echo: false
library(tidyverse)
library(knitr)

# Load saved results
load('q4_simpler_panel_results.RData')

# Display comparison table
kable(comparison_summary,
      digits = 3,
      caption = "Performance comparison: Baseline intersection vs. LASSO-based feature selection")
```

The baseline approach using the intersection of t-test and Random Forest selections yielded a 5-protein panel (DERM, RELT, MRC2, IgD, Cadherin-5) with strong performance: 77.4% accuracy, 81.3% sensitivity, 73.3% specificity, and 88.3% AUC.

LASSO regularization identified a 3-protein panel (DERM, IgD, eIF-4H), achieving 40% complexity reduction (from 5 to 3 proteins). This simpler panel maintained reasonable diagnostic utility with 67.7% accuracy and 75.8% AUC. Notably, two proteins (DERM and IgD) were selected by both approaches, suggesting their robust discriminatory power across methods.

The LASSO panel showed higher specificity (86.7%) but lower sensitivity (50.0%) compared to baseline. This indicates that the simpler model is more conservative, correctly identifying more typically developing (TD) children but missing more ASD cases.

Interestingly, the standard GLM refitted on the LASSO-selected features performed similarly to the regularized LASSO model itself (67.7% accuracy for both, though standard GLM had slightly lower AUC at 71.3%). This suggests that the primary benefit of LASSO in this case was feature selection rather than coefficient shrinkage.

#### Selected proteins and coefficients

```{r}
#| echo: false
library(glmnet)

# Extract and display LASSO coefficients
lasso_coefs <- coef(lasso_fit, s = cv_lasso$lambda.1se)
lasso_coef_df <- data.frame(
  Protein = rownames(lasso_coefs)[which(lasso_coefs != 0)],
  Coefficient = as.numeric(lasso_coefs[which(lasso_coefs != 0)])
) %>%
  arrange(desc(abs(Coefficient)))

kable(lasso_coef_df,
      digits = 4,
      caption = "LASSO-selected proteins and their regularized coefficients")
```

The three selected proteins each contributed meaningfully to classification. DERM (dermatopontin) and IgD (immunoglobulin D) were consistently identified across multiple selection methods, reinforcing their biological relevance. The inclusion of eIF-4H (eukaryotic translation initiation factor 4H) by LASSO but not in the baseline intersection suggests it may capture complementary information when combined with DERM and IgD.

#### Interpretation and clinical implications

This analysis demonstrates a fundamental trade-off in biomarker development: **model complexity versus predictive performance**. The LASSO-based 3-protein panel sacrifices approximately 10 percentage points in accuracy and 12 points in AUC compared to the 5-protein baseline, but achieves a 40% reduction in panel size.

From a clinical perspective, this trade-off may be acceptable in certain contexts:

-   **Screening applications**: Where high specificity is valued to minimize false positives and unnecessary follow-up, the LASSO panel's 86.7% specificity is attractive.

-   **Resource-constrained settings**: Fewer proteins reduce assay costs and complexity, potentially improving accessibility.

-   **Early-stage validation**: A smaller panel may be easier to validate across independent cohorts and platforms.

However, the lower sensitivity (50%) is concerning for diagnostic applications, as it would miss half of true ASD cases. The baseline 5-protein panel's more balanced sensitivity (81.3%) and specificity (73.3%) profile may be more appropriate for clinical diagnosis.

**Key finding**: LASSO successfully identified a minimal 3-protein signature that maintains moderate diagnostic utility, demonstrating that automated regularization methods can extract parsimonious biomarker panels. However, the performance gap suggests that the 5-protein baseline panel may represent a better balance of complexity and accuracy for this dataset. Future work could explore intermediate panel sizes or alternative regularization approaches (e.g., elastic net) to optimize this trade-off.

## Summary

We evaluated serum proteomic biomarkers for distinguishing boys with autism spectrum disorder (ASD) from typically developing (TD) boys, and we quantified how results change under key modeling design choices. We further explored whether a simpler biomarker panel can retain competitive performance.

Data (n = 154; 1,317 proteins) were preprocessed via log10 transformation, z-score standardization, and trimming at ±3 SD, yielding the analysis set `biomarker_clean`. The in-class baseline reproduced a two-stage feature selection (two-sample t-tests and Random Forest importance, top 10 each), followed by a hard intersection and logistic regression evaluation on an 80/20 split. We then implemented three methodological variations: (1) performed all selection exclusively on a training partition and evaluated once on a held-out test set; (2) expanded each selection list to the top 25; and (3) used a fuzzy intersection (union) to combine selections. Finally, to pursue a simpler panel (Task 4), we applied LASSO-regularized logistic regression with 10-fold cross-validation (1-SE rule) using the union of the top-10 lists as candidates, and compared against the baseline.

The replicated baseline (5-protein intersection: DERM, RELT, MRC2, IgD, Cadherin-5) achieved accuracy 77.4% and AUC 88.3%. Variation (1) reduced performance (accuracy 68.8%, AUC 71.5%), reflecting the removal of information leakage when selection is confined to the training set. Variation (2) improved performance (accuracy 83.9%, AUC 95.0%) by allowing more informative features into the intersection (top 25). Variation (3) degraded performance (accuracy 67.7%, AUC 78.3%), indicating that the union introduced more noise than signal. The LASSO approach selected a 3-protein panel (DERM, IgD, eIF-4H) with 40% fewer proteins than baseline and reasonable performance (accuracy 67.7%, AUC 75.8%)—higher specificity but lower sensitivity—illustrating the trade-off between simplicity and accuracy.

The baseline 5-protein intersection delivers the best balance of sensitivity and specificity under the original design, while LASSO demonstrates that substantially simpler panels are feasible with modest loss of accuracy. Smaller panels may suit screening or resource-constrained contexts; for diagnosis, the baseline-sized panel is preferred. Future work should evaluate elastic net regularization and intermediate panel sizes on external cohorts.
